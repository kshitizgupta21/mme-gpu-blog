{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce1c6e6",
   "metadata": {},
   "source": [
    "# Deploy multiple python backend models on Sagemaker using Multi Model Endpoints and Triton Inference Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a59e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953b5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fde551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the image and push it to ECR\n",
    "# build-and-push.sh takes in one arg: the tag. Here we tag the image with 1, but feel free to change the tag\n",
    "# see docker/Dockerfile.sagemaker.gpu for details about the image\n",
    "!cd docker && sudo bash build-and-push.sh 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63b802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker\n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e707f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time \n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess              = boto3.Session()\n",
    "sm                = sess.client('sagemaker')\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role              = get_execution_role()\n",
    "client            = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836d8de",
   "metadata": {},
   "source": [
    "## PyTorch HuggingFace T5 models\n",
    "\n",
    "For a simple use case we will take the pre-trained T5 model from [HuggingFace](https://huggingface.co/transformers/model_doc/t5.html) and deploy it on Sagemaker with Triton as the model server. We used the pre-configured `config.pbtxt` file provided with this repo [here](./travelers_t5_triton/models/t5/config.pbtxt) to specify model [configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) which Triton uses to load the model. We tar the model directory and upload it to s3 to later create a [Sagemaker Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634435a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mme_triton_image_uri='917092859813.dkr.ecr.ap-south-1.amazonaws.com/mme-triton-sagemaker-t5:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507e641",
   "metadata": {},
   "source": [
    "### Packaging model files and uploading to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ccb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C t5_triton/models -czf model.tar.gz t5\n",
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"mme-triton-t5-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $model_uri s3://sagemaker-ap-south-1-917092859813/mme-triton-t5-python/model1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdbecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $model_uri s3://sagemaker-ap-south-1-917092859813/mme-triton-t5-python/model2.tar.gz\n",
    "!aws s3 cp $model_uri s3://sagemaker-ap-south-1-917092859813/mme-triton-t5-python/model3.tar.gz\n",
    "!aws s3 cp $model_uri s3://sagemaker-ap-south-1-917092859813/mme-triton-t5-python/model4.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca9531",
   "metadata": {},
   "source": [
    "### Create Sagemaker Enpoint\n",
    "\n",
    "We start off by creating a [sagemaker model](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html) from the model files we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also provide an additional Environment Variable i.e. `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME` which specifies the name of the model to be loaded by Triton. **The value of this key should match the folder name in the model package uploaded to s3**. This variable is optional in case of a single model. In case of ensemble models, this key **has to be** specified for Triton to startup in Sagemaker.\n",
    "\n",
    "*Note*: The current release of Triton (21.06-py3) on Sagemaker doesn't support running instances of different models on the same server, except in case of [ensembles](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md#ensemble-models). Only multiple model instances of the same model are supported, which can be specified under the [instance-groups](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#instance-groups) section of the config.pbtxt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://sagemaker-ap-south-1-917092859813/mme-triton-t5-python/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = 'mme-triton-t5-python-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_url,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"t5\", \n",
    "                    \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216\",\n",
    "                   \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName         = sm_model_name,\n",
    "    ExecutionRoleArn  = role,\n",
    "    PrimaryContainer  = container)\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bceab3c",
   "metadata": {},
   "source": [
    "Using the model above, we create an [endpoint configuration](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html) where we can specify the type and number of instances we want in the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b63f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'mme-triton-t5-python-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants = [{\n",
    "        'InstanceType'        : 'ml.g4dn.xlarge',\n",
    "        'InitialVariantWeight': 1,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName'           : sm_model_name,\n",
    "        'VariantName'         : 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd847aa",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'mme-triton-t5-python-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName         = endpoint_name,\n",
    "    EndpointConfigName   = endpoint_config_name)\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67510a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8f445",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "\n",
    "Once we have the endpoint running we can use the [sample image](./kitten.jpg) provided to do an inference using json as the payload format. For inference request format, Triton uses the KFServing community standard [inference protocols](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e6492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c59179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# print(tokenizer)\n",
    "input_ids = tokenizer(\"summarize: SageMaker enables customers to deploy a model using custom code with NVIDIA Triton Inference Server. This functionality is available through the development of Triton Inference Server Containers. These containers include NVIDIA Triton Inference Server, support for common ML frameworks, and useful environment variables that let you optimize performance on SageMaker. For a list of all available Deep Learning Containers images, see Available Deep Learning Containers Images. Deep Learning Containers images are maintained and regularly updated with security patches.\", return_tensors='pt').input_ids\n",
    "# print(input_ids.numpy().astype(np.int32).shape)\n",
    "\n",
    "input_data = input_ids.numpy().astype(np.int32)\n",
    "\n",
    "input_name = 'input'\n",
    "output_name = \"output\"\n",
    "inputs = []\n",
    "outputs = []\n",
    "inputs.append(httpclient.InferInput(input_name, input_data.shape, \"INT32\"))\n",
    "inputs[0].set_data_from_numpy(input_data, binary_data=True)\n",
    "outputs.append(\n",
    "    httpclient.InferRequestedOutput(output_name, binary_data=True))\n",
    "request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "    inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935bd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                 TargetModel='model.tar.gz')\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response['Body'].read(), header_length=int(header_length_str))\n",
    "output_data = result.as_numpy(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfc4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.decode(\n",
    "            output_data[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f140a4d",
   "metadata": {},
   "source": [
    "#### BenchMark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f39f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!for i in {1..5}; do aws s3 cp s3://sagemaker-ap-south-1-917092859813/mme-triton-t5-python/model.tar.gz s3://sagemaker-ap-south-1-917092859813/mme-triton-t5-python/t5_python_v\"$i\".tar.gz; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0bbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform auto-scaling of the endpoint based on GPU memory utilization\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "# Define application auto-scaling client\n",
    "# Common class representing Application Auto Scaling for SageMaker amongst other AWS services\n",
    "auto_scaling_client = boto3.client('application-autoscaling')\n",
    "\n",
    "# This is the format in which application autoscaling references the endpoint\n",
    "resource_id='endpoint/' + endpoint_name + '/variant/' + 'AllTraffic' \n",
    "response = auto_scaling_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=2\n",
    ")\n",
    "\n",
    "\n",
    "# GPUMemoryUtilization metric\n",
    "response = auto_scaling_client.put_scaling_policy(\n",
    "    PolicyName='GPUMemoryUtil-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', # SageMaker supports only Instance Count\n",
    "    PolicyType='TargetTrackingScaling', # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        # Scale out when GPU Memory utilization hits GPUMemoryUtilization target value.\n",
    "        'TargetValue': 50.0, \n",
    "        'CustomizedMetricSpecification':\n",
    "        {\n",
    "            'MetricName': 'GPUMemoryUtilization',\n",
    "            'Namespace': '/aws/sagemaker/Endpoints',\n",
    "            'Dimensions': [\n",
    "                {'Name': 'EndpointName', 'Value': endpoint_name },\n",
    "                {'Name': 'VariantName','Value': 'AllTraffic'}\n",
    "            ],\n",
    "            'Statistic': 'Average', # Possible - 'Statistic': 'Average'|'Minimum'|'Maximum'|'SampleCount'|'Sum'\n",
    "            'Unit': 'Percent'\n",
    "        },\n",
    "        'ScaleInCooldown': 600,\n",
    "        'ScaleOutCooldown': 200 \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    n = random.randint(1,3)\n",
    "    model_name=f\"t5_python_v{n}.tar.gz\"\n",
    "    print(model_name)\n",
    "\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                 TargetModel=model_name)\n",
    "\n",
    "    # Parse json header size length from the response\n",
    "    header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "    header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "\n",
    "    # Read response body\n",
    "    result = httpclient.InferenceServerClient.parse_response_body(\n",
    "        response['Body'].read(), header_length=int(header_length_str))\n",
    "    output_data = result.as_numpy(output_name)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(\n",
    "            output_data[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "    print(decoded_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4a71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad06931f",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459151bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcf3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
