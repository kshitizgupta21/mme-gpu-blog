{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f998b857",
   "metadata": {},
   "source": [
    "# Serve Multiple DL models on GPU with Amazon SageMaker Multi-model endpoints (MME)\n",
    "\n",
    "\n",
    "\n",
    "Amazon SageMaker multi-model endpoints(MME) provide a scalable and cost-effective way to deploy large number of deep learning models. Previously, customers had limited options to deploy 100s of deep learning models that need accelerated compute with GPUs. Now customers can deploy 1000s of deep learning models behind one SageMaker endpoint. Now, MME will run multiple models on a GPU core, share GPU instances behind an endpoint across multiple models and dynamically load/unload models based on the incoming traffic. With this, customers can significantly save cost and achieve best price performance.\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Note </strong>\n",
    "This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g5.xlarge`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a5aea",
   "metadata": {},
   "source": [
    "In this notebook, we will walk you through how to use NVIDIA Triton Inference Server on Amazon SageMaker MME with GPU feature to deploy two different NLP models (**DistilBERT** and **T5**) for two different use-cases (**Classification** and **Summarization**) in two different frameworks (**TensorFlow** and **PyTorch**) on the same GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19981028",
   "metadata": {},
   "source": [
    "## Installs\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server. Update SageMaker, boto3, awscli etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c5f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker\n",
    "!pip install nvidia-pyindex --quiet\n",
    "!pip install tritonclient[http] --quiet\n",
    "!pip install transformers[sentencepiece] --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de8cae",
   "metadata": {},
   "source": [
    "## Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15704d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"nlp-mme-gpu\"\n",
    "\n",
    "# account mapping for SageMaker MME Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.09-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa967afc",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "This section presents overview of main steps for preparing DistilBERT TensorFlow model (served using TensorFlow backend) and T5 Pytorch (served using Python backend) model to be served using Triton Inference Server.\n",
    "### 1. Generate Model Artifacts\n",
    "\n",
    "#### DistilBERT TensorFlow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e9d6c",
   "metadata": {},
   "source": [
    "First, we use HuggingFace transformers to load pre-trained DistilBERT TensorFlow model that has been fine-tuned for sentiment analysis binary classification task. Then, we save the model as SavedModel serialized format. The `generate_distilbert_tf.sh` bash script performs all these steps inside the NGC TensorFlow container. Run the command below to generate DistilBERT Tensorflow model. It can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd573974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================\n",
      "== TensorFlow ==\n",
      "================\n",
      "\n",
      "NVIDIA Release 22.09-tf2 (build 44878075)\n",
      "TensorFlow Version 2.9.1\n",
      "\n",
      "Container image Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "Copyright 2017-2022 The TensorFlow Authors.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "NOTE: CUDA Forward Compatibility mode ENABLED.\n",
      "  Using CUDA 11.8 driver version 520.61.03 with kernel driver version 510.47.03.\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\n",
      "\n",
      "Installing Transformers...\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading pretrained DistilBERT Classification TF Model from HuggingFace...\n",
      "Downloading: 100%|██████████████████████████████| 629/629 [00:00<00:00, 731kB/s]\n",
      "Downloading: 100%|███████████████████████████| 268M/268M [00:08<00:00, 30.3MB/s]\n",
      "Serializing Model to SavedModel file...\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses, add_layer_call_fn while saving (showing 5 of 166). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "!docker run --gpus=all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --rm -it \\\n",
    "            -v `pwd`/workspace:/workspace nvcr.io/nvidia/tensorflow:22.09-tf2-py3 \\\n",
    "            /bin/bash generate_distilbert_tf.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dbdda1",
   "metadata": {},
   "source": [
    "#### T5 PyTorch Model\n",
    "\n",
    "In case of T5-small HuggingFace PyTorch Model, since we are serving it using Triton's [python backend](https://github.com/triton-inference-server/python_backend#usage) we have python script [model.py](./workspace/model.py) which implements all the logic to initialize the T5 model and execute inference for the summarization task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430443ac",
   "metadata": {},
   "source": [
    "### 2. Build Model Respository\n",
    "\n",
    "Using Triton on SageMaker requires us to first set up a [model repository](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md) folder containing the models we want to serve. For each model we need to create a model directory consisting of the model artifact and define config.pbtxt file to specify [model configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) which Triton uses to load and serve the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda732c3",
   "metadata": {},
   "source": [
    "#### DistilBERT TensorFlow Model\n",
    "\n",
    "Model repository structure for DistilBERT TensorFlow Model.\n",
    "\n",
    "```\n",
    "distilbert_tf\n",
    "├── 1\n",
    "│   └── model.savedmodel\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "Model configuration must specify the platform and backend properties, max_batch_size property and the input and output tensors of the model. Additionally, you can specify instance_group and dynamic_batching properties for optimal inference performance in terms of latency and concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaab5ae",
   "metadata": {},
   "source": [
    "Below we set up the DistilBERT TensorFlow Model in the model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6a0a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/distilbert_tf/1\n",
    "!cp -r workspace/hf_distilbert/saved_model/1 workspace/model.savedmodel\n",
    "!cp -r workspace/model.savedmodel model_repository/distilbert_tf/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462beaf4",
   "metadata": {},
   "source": [
    "Then we define its config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4257cd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_repository/distilbert_tf/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_repository/distilbert_tf/config.pbtxt\n",
    "name: \"distilbert_tf\"\n",
    "platform: \"tensorflow_savedmodel\"\n",
    "max_batch_size: 8\n",
    "input: [\n",
    "    {\n",
    "        name: \"input_ids\"\n",
    "        data_type: TYPE_INT32\n",
    "        dims: [ -1 ]\n",
    "    },\n",
    "    {\n",
    "        name: \"attention_mask\"\n",
    "        data_type: TYPE_INT32\n",
    "        dims: [ -1 ]\n",
    "    }\n",
    "]\n",
    "output: [\n",
    "    {\n",
    "        name: \"logits\"\n",
    "        data_type: TYPE_FP32\n",
    "        dims: [ 2 ]\n",
    "    }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n",
    "dynamic_batching {\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf4bef",
   "metadata": {},
   "source": [
    "#### T5 Python Backend Model\n",
    "\n",
    "Model repository structure for T5 Model.\n",
    "\n",
    "```\n",
    "t5_pytorch\n",
    "├── 1\n",
    "│   └── model.py\n",
    "└── config.pbtxt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f33e14",
   "metadata": {},
   "source": [
    "Next we set up the T5 PyTorch Python Backend Model in the model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06011de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/t5_pytorch/1\n",
    "!cp workspace/model.py model_repository/t5_pytorch/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c663655",
   "metadata": {},
   "source": [
    "##### Create Conda Environment for Dependencies\n",
    "\n",
    "For serving the HuggingFace T5 PyTorch Model using Triton's Python backend we have PyTorch and HuggingFace transformers as dependencies.\n",
    "\n",
    "We follow the instructions from the [Triton documentation for packaging dependencies](https://github.com/triton-inference-server/python_backend#2-packaging-the-conda-environment) to be used in the python backend as conda env tar file. Running the bash script [create_hf_env.sh]('./workspace/create_hf_env.sh') creates the conda environment containing PyTorch and HuggingFace transformers, packages it as tar file and then we move it into the t5-pytorch model directory. This can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be5ca00c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 22.9.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/hf_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.8\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge\n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu\n",
      "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4\n",
      "  ca-certificates    conda-forge/linux-64::ca-certificates-2022.9.24-ha878542_0\n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.39-hc81fddc_0\n",
      "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5\n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-12.2.0-h65d4601_19\n",
      "  libgomp            conda-forge/linux-64::libgomp-12.2.0-h65d4601_19\n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0\n",
      "  libsqlite          conda-forge/linux-64::libsqlite-3.39.4-h753d276_0\n",
      "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h7f98852_1000\n",
      "  libzlib            conda-forge/linux-64::libzlib-1.2.13-h166bdaf_4\n",
      "  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1\n",
      "  openssl            conda-forge/linux-64::openssl-3.0.7-h166bdaf_0\n",
      "  pip                conda-forge/noarch::pip-22.3.1-pyhd8ed1ab_0\n",
      "  python             conda-forge/linux-64::python-3.8.13-ha86cf86_0_cpython\n",
      "  readline           conda-forge/linux-64::readline-8.1.2-h0f457ee_0\n",
      "  setuptools         conda-forge/noarch::setuptools-65.5.1-pyhd8ed1ab_0\n",
      "  sqlite             conda-forge/linux-64::sqlite-3.39.4-h4ff8645_0\n",
      "  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0\n",
      "  wheel              conda-forge/noarch::wheel-0.38.4-pyhd8ed1ab_0\n",
      "  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate hf_env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pypi.ngc.nvidia.com, https://download.pytorch.org/whl/cu116\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.0%2Bcu116-cp38-cp38-linux_x86_64.whl (1983.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "Successfully installed torch-1.13.0+cu116 typing-extensions-4.4.0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers[sentencepiece]\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-1.23.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m296.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m162.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m368.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m338.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m273.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m266.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m359.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<=3.20.2\n",
      "  Downloading protobuf-3.20.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m362.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m293.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/hf_env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers[sentencepiece]) (4.4.0)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m315.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.1/161.1 kB\u001b[0m \u001b[31m372.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m322.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m270.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Installing collected packages: tokenizers, sentencepiece, urllib3, tqdm, regex, pyyaml, pyparsing, protobuf, numpy, idna, filelock, charset-normalizer, certifi, requests, packaging, huggingface-hub, transformers\n",
      "Successfully installed certifi-2022.9.24 charset-normalizer-2.1.1 filelock-3.8.0 huggingface-hub-0.11.0 idna-3.4 numpy-1.23.4 packaging-21.3 protobuf-3.20.2 pyparsing-3.0.9 pyyaml-6.0 regex-2022.10.31 requests-2.28.1 sentencepiece-0.1.97 tokenizers-0.13.2 tqdm-4.64.1 transformers-4.24.0 urllib3-1.26.12\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pypi.ngc.nvidia.com\n",
      "Collecting conda-pack\n",
      "  Downloading conda-pack-0.6.0.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/hf_env/lib/python3.8/site-packages (from conda-pack) (65.5.1)\n",
      "Building wheels for collected packages: conda-pack\n",
      "  Building wheel for conda-pack (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for conda-pack: filename=conda_pack-0.6.0-py2.py3-none-any.whl size=30882 sha256=74153f17cbca45dd510a6b31c7f48c486d6e32534b0e0127d4e2eca0b4ffc2e9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dnpq8lo4/wheels/ad/ce/cd/58348f78b175becab147bbc179095c7f9c7624f7aad28cbd6a\n",
      "Successfully built conda-pack\n",
      "Installing collected packages: conda-pack\n",
      "Successfully installed conda-pack-0.6.0\n",
      "Collecting packages...\n",
      "Packing environment at '/home/ec2-user/anaconda3/envs/hf_env' to 'hf_env.tar.gz'\n",
      "[########################################] | 100% Completed |  2min 25.8s\n"
     ]
    }
   ],
   "source": [
    "!bash workspace/create_hf_env.sh\n",
    "!mv hf_env.tar.gz model_repository/t5_pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576e9cb",
   "metadata": {},
   "source": [
    "After creating the tar file from the conda environment and placing it in model folder, you need to tell Python backend to use that environment for your model. We do this by including the lines below in the model `config.pbtxt` file:\n",
    "\n",
    "```\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"$$TRITON_MODEL_DIRECTORY/hf_env.tar.gz\"}\n",
    "}\n",
    "```\n",
    "Here, `$$TRITON_MODEL_DIRECTORY` helps provide environment path relative to the model folder in model repository and is resolved to `$pwd/model_repository/t5_pytorch`. Finally `hf_env.tar.gz` is the name we gave to our conda env file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214ffb2",
   "metadata": {},
   "source": [
    "Now we are ready to define the config file for t5 pytorch model being served through Triton's Python Backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8286b77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_repository/t5_pytorch/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_repository/t5_pytorch/config.pbtxt\n",
    "name: \"t5_pytorch\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 8\n",
    "input: [\n",
    "    {\n",
    "        name: \"input_ids\"\n",
    "        data_type: TYPE_INT32\n",
    "        dims: [ -1 ]\n",
    "    },\n",
    "    {\n",
    "        name: \"attention_mask\"\n",
    "        data_type: TYPE_INT32\n",
    "        dims: [ -1 ]\n",
    "    }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n",
    "dynamic_batching {\n",
    "}\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"$$TRITON_MODEL_DIRECTORY/hf_env.tar.gz\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c1a3a",
   "metadata": {},
   "source": [
    "### 3. Package models and upload to S3\n",
    "\n",
    "Next, we will package our models as `*.tar.gz` files for uploading to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d82778b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C model_repository/ -czf distilbert_tf.tar.gz distilbert_tf\n",
    "model_uri_distilbert_tf = sagemaker_session.upload_data(path=\"distilbert_tf.tar.gz\", key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbfb8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C model_repository/ -czf t5_pytorch.tar.gz t5_pytorch\n",
    "model_uri_t5_pytorch = sagemaker_session.upload_data(path=\"t5_pytorch.tar.gz\", key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d5858",
   "metadata": {},
   "source": [
    "### 4. Create SageMaker Endpoint\n",
    "\n",
    "Now that we have uploaded the model artifacts to S3, we can create a SageMaker multi-model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306205d6",
   "metadata": {},
   "source": [
    "#### Define the serving container\n",
    "In the container definition, define the `ModelDataUrl` to specify the S3 directory that contains all the models that SageMaker multi-model endpoint will use to load and serve predictions. Set `Mode` to `MultiModel` to indicate SageMaker would create the endpoint with MME container specifications. We set the container with an image that supports deploying multi-model endpoints with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e0c6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "\n",
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_url,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd976b",
   "metadata": {},
   "source": [
    "#### Create a multi-model object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ca733",
   "metadata": {},
   "source": [
    "Once the image, data location are set we create the model using `create_model` by specifying the `ModelName` and the Container definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1a0f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-west-2:354625738399:model/nlp-mme-gpu-mdl-2022-11-16-22-37-02\n"
     ]
    }
   ],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ae921",
   "metadata": {},
   "source": [
    "#### Define configuration for the multi-model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab3282",
   "metadata": {},
   "source": [
    "Using the model above, we create an [endpoint configuration](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html) where we can specify the type and number of instances we want in the endpoint. Here we are deploying to `g5.xlarge` NVIDIA GPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3abaf76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint-config/nlp-mme-gpu-epc-2022-11-16-22-37-02\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ced92f",
   "metadata": {},
   "source": [
    "#### Create Multi-Model Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e5d0e",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edf5b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint/nlp-mme-gpu-ep-2022-11-16-22-37-02\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8590807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint/nlp-mme-gpu-ep-2022-11-16-22-37-02\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ea7e7",
   "metadata": {},
   "source": [
    "### 5. Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77055746",
   "metadata": {},
   "source": [
    "Once we have the endpoint running we can use some sample raw data to do an inference using JSON as the payload format. For the inference request format, Triton uses the KFServing community standard [inference protocols](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59474865",
   "metadata": {},
   "source": [
    "#### Add utility methods for preparing JSON request payload\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a5e9a",
   "metadata": {},
   "source": [
    "We'll use the following utility methods to convert our inference request for DistilBERT and T5 models into a json payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "582e5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.model_max_length = 128\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_text(model_name, text):\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    tokenized_text = tokenizer(text, padding=\"max_length\", return_tensors=\"np\")\n",
    "    return tokenized_text.input_ids, tokenized_text.attention_mask\n",
    "\n",
    "def get_text_payload(model_name, text):\n",
    "    input_ids, attention_mask = tokenize_text(model_name, text)\n",
    "    payload = {}\n",
    "    payload[\"inputs\"] = []\n",
    "    payload[\"inputs\"].append({\"name\": \"input_ids\", \"shape\": input_ids.shape, \"datatype\": \"INT32\", \"data\": input_ids.tolist()})\n",
    "    payload[\"inputs\"].append({\"name\": \"attention_mask\", \"shape\": attention_mask.shape, \"datatype\": \"INT32\", \"data\": attention_mask.tolist()})\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936089c7",
   "metadata": {},
   "source": [
    "#### Invoke target model on Multi Model Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e38322",
   "metadata": {},
   "source": [
    "We can send inference request to multi-model endpoint using `invoke_enpoint` API. We specify the `TargetModel` in the invocation call and pass in the payload for each model type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491e97c",
   "metadata": {},
   "source": [
    "#### DistilBERT TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428aea45",
   "metadata": {},
   "source": [
    "##### Sample DistilBERT Inference using Json Payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b8afc",
   "metadata": {},
   "source": [
    "First, we show some sample inference on the DistilBERT TensorFlow Binary Classification Model deployed on Triton's TensorFlow SavedModel Backend behind SageMaker MME GPU endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad68159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_classify = [\"Many critics thought the sequel film was unnecessary\"]\n",
    "batch_size = len(texts_to_classify)\n",
    "\n",
    "distilbert_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "distilbert_payload = get_text_payload(distilbert_model, texts_to_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffcf56e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(distilbert_payload),\n",
    "    TargetModel=\"distilbert_tf.tar.gz\",\n",
    ")\n",
    "\n",
    "response_body = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "logits = np.array(response_body[\"outputs\"][0][\"data\"]).reshape(batch_size, -1)\n",
    "CLASSES = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "predictions = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    pred_class_idx = np.argmax(logits[i])\n",
    "    predictions.append(CLASSES[pred_class_idx])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c25def",
   "metadata": {},
   "source": [
    "##### Sample DistilBERT Inference using Binary + Json Payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad10fd",
   "metadata": {},
   "source": [
    "We can also use `binary+json` as the payload format to get better performance for the inference call. The specification of this format is provided [here](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md).\n",
    "\n",
    "**Note:** With the binary+json format, we have to specify the length of the request metadata in the header to allow Triton to correctly parse the binary payload. This is done using a custom Content-Type header `Application/vnd.sagemaker-triton.binary+json;json-header-size={}`.\n",
    "\n",
    "Please note, this is different from using `Inference-Header-Content-Length` header on a stand-alone Triton server since custom headers are not allowed in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edf2b5",
   "metadata": {},
   "source": [
    "The `tritonclient` package in Triton provides utility methods to generate the payload without having to know the details of the specification. We'll use the following method to convert our inference request for DistilBERT and T5 models into a binary format which provides lower latencies for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6adc378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "\n",
    "def get_text_payload_binary(model_name, text):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_ids, attention_mask = tokenize_text(model_name, text)\n",
    "    inputs.append(httpclient.InferInput(\"input_ids\", input_ids.shape, \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(\"attention_mask\", attention_mask.shape, \"INT32\"))\n",
    "\n",
    "    inputs[0].set_data_from_numpy(input_ids.astype(np.int32), binary_data=True)\n",
    "    inputs[1].set_data_from_numpy(attention_mask.astype(np.int32), binary_data=True)\n",
    "    \n",
    "    output_name = \"output\" if model_name == \"t5-small\" else \"logits\"\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df2de79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "request_body, header_length = get_text_payload_binary(distilbert_model, texts_to_classify)\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                  TargetModel='distilbert_tf.tar.gz')\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "output_name = \"logits\"\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response['Body'].read(), header_length=int(header_length_str))\n",
    "logits = result.as_numpy(output_name)\n",
    "CLASSES = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "predictions = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    pred_class_idx = np.argmax(logits[i])\n",
    "    predictions.append(CLASSES[pred_class_idx])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d5d14",
   "metadata": {},
   "source": [
    "#### T5 PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc1ad5",
   "metadata": {},
   "source": [
    "Next, we show some sample inference for summarization on the T5 PyTorch Model deployed on Triton's Python Backend behind SageMaker MME GPU endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "affbd657",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_summarize = [\n",
    "    \"summarize: SageMaker enables customers to deploy a model using custom code with NVIDIA Triton Inference Server. This functionality is available through the development of Triton Inference Server Containers. These containers include NVIDIA Triton Inference Server, support for common ML frameworks, and useful environment variables that let you optimize performance on SageMaker. For a list of all available Deep Learning Containers images, see Available Deep Learning Containers Images. Deep Learning Containers images are maintained and regularly updated with security patches\",\n",
    "]\n",
    "batch_size = len(texts_to_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c95ab6",
   "metadata": {},
   "source": [
    "##### Sample T5 Inference using Json Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b870fe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker enables customers to deploy a model using custom code. these containers include NVIDIA Triton Inference Server, support for common ML frameworks, and useful environment variables that let you optimize performance on SageMaker. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "t5_payload = get_text_payload(\"t5-small\", texts_to_summarize)\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(t5_payload),\n",
    "    TargetModel=\"t5_pytorch.tar.gz\",\n",
    ")\n",
    "\n",
    "response_body = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "output_ids = np.array(response_body[\"outputs\"][0][\"data\"]).reshape(batch_size, -1)\n",
    "t5_tokenizer = get_tokenizer(\"t5-small\")\n",
    "decoded_outputs = t5_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "for text in decoded_outputs:\n",
    "    print(text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1e762",
   "metadata": {},
   "source": [
    "##### Sample T5 Inference using Binary + Json Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1bcc5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker enables customers to deploy a model using custom code. these containers include NVIDIA Triton Inference Server, support for common ML frameworks, and useful environment variables that let you optimize performance on SageMaker. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "request_body, header_length = get_text_payload_binary(\"t5-small\", texts_to_summarize)\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                 TargetModel='t5_pytorch.tar.gz')\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "output_name = \"output\"\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response['Body'].read(), header_length=int(header_length_str))\n",
    "output_ids = result.as_numpy(output_name)\n",
    "decoded_output = t5_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "for text in decoded_outputs:\n",
    "    print(text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6cc08",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61b80397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'e6018dce-957b-40cc-87f3-8d84893a9fe4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e6018dce-957b-40cc-87f3-8d84893a9fe4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 16 Nov 2022 22:06:01 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_model(ModelName=sm_model_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d0945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
